# HyperCNN  
**Hyperbolic-topology convolutions for ultra-short paths**  
**Convoluciones con topologÃ­a hiperbÃ³lica para caminos ultra-cortos**

---

## ğŸ‡ºğŸ‡¸ English Version

Deep CNNs have achieved outstanding results in image classification, but often with heavy compute and memory costs. Residual networks improved gradient flow, but they donâ€™t ensure globally efficient connectivity. Transformers, while powerful, scale quadratically and arenâ€™t suited for edge deployment.

**HyperCNN** introduces a mathematically designed connectivity pattern inspired by the small-world phenomenon. Any block can reach any other in just **2â€“4 hops**, without attention, tokens, or graph ops â€” fully compatible with classic, efficient CNN implementations.

---

## ğŸš€ Highlights

### ğŸ”­ Hyperbolic Topology  
A deterministic base-4 indexing system guarantees a tiny network diameter:  
**D â‰¤ logâ‚„(N)**

### âš¡ Efficient Implementation  
Zero inference overhead â€” all connections are precomputed and mapped to standard convolutions + tensor additions.

### ğŸ† Performance  
Outperforms **MobileNetV2** and **ShuffleNetV2** on CIFAR-10 with comparable or fewer parameters.

### ğŸ”§ Compatibility  
Fully supports **PyTorch**, **mixed precision**, and **ONNX export**.

---

## ğŸ§  2. Architecture

### 2.1 Short-Path Connectivity  
Each of the **N blocks** receives a 4-digit base-4 identifier (example: `[0,1,3,2]`).  
Two blocks connect if they share **at least one digit in the same position**.

**Examples**

- `[0,0,0]` vs `[3,3,3]` â†’ no shared digits â†’ âŒ no connection  
- `[1,1,1]` vs `[2,0,0]` â†’ no shared digits â†’ âŒ no connection  
- `[1,1,1]` vs `[2,1,0]` â†’ share â€œ1â€ â†’ âœ”ï¸ connected

**Benefits**

- High local clustering  
- Short global paths: **â‰¤ âŒˆlogâ‚„(N)âŒ‰**  
- Fully deterministic  

### ğŸ”‘ Network Diameter

| N Blocks | Diameter D |
|---------|------------|
| 16      | 2          |
| 32      | 3          |
| 64      | 3          |

---

# ğŸ‡ªğŸ‡¸ VersiÃ³n en EspaÃ±ol

Las CNN profundas han logrado resultados fuertes en clasificaciÃ³n de imÃ¡genes, pero suelen requerir bastante cÃ³mputo y memoria. ResNet ayudÃ³ con el flujo de gradiente, pero no garantiza eficiencia global. Transformers, aunque potentes, escalan cuadrÃ¡ticamente y no son ideales para dispositivos edge.

**HyperCNN** propone una conectividad diseÃ±ada matemÃ¡ticamente, inspirada en el fenÃ³meno *small-world*: cualquier bloque puede alcanzar a cualquier otro en **2â€“4 saltos**, sin atenciÃ³n, sin tokens y sin operaciones especiales. Todo funciona con convoluciones clÃ¡sicas y eficientes.

---

## ğŸš€ Highlights

### ğŸ”­ TopologÃ­a HiperbÃ³lica  
Un sistema determinista basado en Ã­ndices en base 4 asegura un diÃ¡metro mÃ­nimo:  
**D â‰¤ logâ‚„(N)**

### âš¡ ImplementaciÃ³n Eficiente  
No aÃ±ade costo en inferencia: las conexiones se precomputan y usan convoluciones estÃ¡ndar + sumas de tensores.

### ğŸ† Rendimiento  
Supera a **MobileNetV2** y **ShuffleNetV2** en CIFAR-10 con un nÃºmero de parÃ¡metros comparable o menor.

### ğŸ”§ Compatibilidad  
Compatible con **PyTorch**, **mixed precision** y **exportaciÃ³n ONNX**.

---

## ğŸ§  2. Arquitectura

### 2.1 Conectividad de Camino Corto  
A cada uno de los **N bloques** se le asigna un identificador de 4 dÃ­gitos en base 4 (ejemplo: `[0,1,3,2]`).  
Dos bloques se conectan si comparten **al menos un dÃ­gito en la misma posiciÃ³n**.

**Ejemplos**

- `[0,0,0]` vs `[3,3,3]` â†’ sin coincidencias â†’ âŒ sin conexiÃ³n  
- `[1,1,1]` vs `[2,0,0]` â†’ sin coincidencias â†’ âŒ sin conexiÃ³n  
- `[1,1,1]` vs `[2,1,0]` â†’ comparten â€œ1â€ â†’ âœ”ï¸ conexiÃ³n

**Ventajas**

- Alta cohesiÃ³n local  
- Caminos globales cortos: **â‰¤ âŒˆlogâ‚„(N)âŒ‰**  
- Arquitectura completamente determinista  

### ğŸ”‘ DiÃ¡metro de Red

| N Bloques | DiÃ¡metro D |
|-----------|------------|
| 16        | 2          |
| 32        | 3          |
| 64        | 3          |

N = 32 â†’ D = 3

N = 64 â†’ D = 3
